{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ciaYhf8Lyuk",
        "outputId": "70a133ef-e1c4-4819-a961-a36b9e9d3bb6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/google')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul2o8SbdonfQ",
        "outputId": "2fd28011-8c4b-4ab9-8391-fa710d3b99f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        texts = df.text.values.tolist()\n",
        "        texts = [self._preprocess(text) for text in texts]\n",
        "        self._print_random_samples(texts)\n",
        "        self.texts = [texts]\n",
        "        if 'target' in df:\n",
        "            classes = df.target.values.tolist()\n",
        "            self.labels = classes\n",
        "\n",
        "    def _print_random_samples(self, texts):\n",
        "        np.random.seed(42)\n",
        "        random_entries = np.random.randint(0, len(texts), 5)\n",
        "\n",
        "        for i in random_entries:\n",
        "            print(f\"Entry {i}: {texts[i]}\")\n",
        "\n",
        "\n",
        "    def _preprocess(self, text):\n",
        "        text = self._remove_amp(text)\n",
        "        text = self._remove_links(text)\n",
        "        text = self._remove_hashes(text)\n",
        "        text = self._remove_retweets(text)\n",
        "        text = self._remove_mentions(text)\n",
        "        text = self._remove_multiple_spaces(text)\n",
        "\n",
        "        # text = self._lowercase(text)\n",
        "        text = self._remove_punctuation(text)\n",
        "        # text = self._remove_numbers(text)\n",
        "\n",
        "        text_tokens = self._tokenize(text)\n",
        "        text_tokens = self._stopword_filtering(text_tokens)\n",
        "        # text_tokens = self._stemming(text_tokens)\n",
        "        text = self._stitch_text_tokens_together(text_tokens)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def _remove_amp(self, text):\n",
        "        return text.replace(\"&amp;\", \" \")\n",
        "\n",
        "    def _remove_mentions(self, text):\n",
        "        return re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    def _remove_multiple_spaces(self, text):\n",
        "        return re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    def _remove_retweets(self, text):\n",
        "        return re.sub(r'^RT[\\s]+', ' ', text)\n",
        "\n",
        "    def _remove_links(self, text):\n",
        "        return re.sub(r'https?:\\/\\/[^\\s\\n\\r]+', ' ', text)\n",
        "\n",
        "    def _remove_hashes(self, text):\n",
        "        return re.sub(r'#', ' ', text)\n",
        "\n",
        "    def _stitch_text_tokens_together(self, text_tokens):\n",
        "        return \" \".join(text_tokens)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return nltk.word_tokenize(text, language=\"english\")\n",
        "\n",
        "    def _stopword_filtering(self, text_tokens):\n",
        "        stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "        return [token for token in text_tokens if token not in stop_words]\n",
        "\n",
        "    def _stemming(self, text_tokens):\n",
        "        porter = nltk.stem.porter.PorterStemmer()\n",
        "        return [porter.stem(token) for token in text_tokens]\n",
        "\n",
        "    def _remove_numbers(self, text):\n",
        "        return re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "    def _lowercase(self, text):\n",
        "        return text.lower()\n",
        "\n",
        "    def _remove_punctuation(self, text):\n",
        "        return ''.join(character for character in text if character not in string.punctuation)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        label = -1\n",
        "        if hasattr(self, 'labels'):\n",
        "            label = self.labels[idx]\n",
        "        return text, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQhIpHcBpvEP",
        "outputId": "30639062-913f-4f08-f2da-e9ef61764397"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "test_project = pd.read_csv('./tweets.csv',  on_bad_lines='skip',delimiter=\",\",\n",
        "              header=0, encoding='ISO-8859-1')\n",
        "test_project = test_project[test_project['language']=='en'][['date','content']]\n",
        "print(test_project.head(5))\n",
        "test_project.rename(columns={'content':'text'}, inplace=True)\n",
        "test_project['date'] = pd.to_datetime(test_project['date'])\n",
        "print(len(test_project))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MC0eaVIrbxz",
        "outputId": "56cbfe84-363f-4ea6-9c73-b1c78908aaee"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-fd0f85fff917>:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  test_project = pd.read_csv('./tweets.csv',  on_bad_lines='skip',delimiter=\",\",\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        date  \\\n",
            "1  2023-02-21 03:29:07+00:00   \n",
            "2  2023-02-21 03:29:04+00:00   \n",
            "3  2023-02-21 03:28:06+00:00   \n",
            "5  2023-02-21 03:27:27+00:00   \n",
            "6  2023-02-21 03:27:11+00:00   \n",
            "\n",
            "                                             content  \n",
            "1  New search &amp; rescue work is in progress in...  \n",
            "2  Can't imagine those who still haven't recovere...  \n",
            "3  its a highkey sign for all of us to ponder ove...  \n",
            "5  See how strong was the #Earthquake of Feb 20, ...  \n",
            "6  More difficult news today on top of struggles ...  \n",
            "189626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_project.head(10)"
      ],
      "metadata": {
        "id": "wAlGzWdExU3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "three_days = datetime.date(2023, 2, 8)\n",
        "one_week = datetime.date(2023, 2, 12)\n",
        "test_3d = test_project[test_project['date'].dt.date<=three_days]\n",
        "test_1w = test_project[test_project['date'].dt.date<=one_week][test_project['date'].dt.date>three_days]\n",
        "test_after_1w = test_project[test_project['date'].dt.date>one_week]\n",
        "print(len(test_3d), len(test_1w), len(test_after_1w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnNuMq-qrer1",
        "outputId": "bf8215e1-e901-4f29-fc58-349b7fc9a837"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130125 49307 10194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-563cf3a3b788>:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  test_1w = test_project[test_project['date'].dt.date<=one_week][test_project['date'].dt.date>three_days]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TweebankNLP/bertweet-tb2_wnut17-ner\")\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"TweebankNLP/bertweet-tb2_wnut17-ner\")\n",
        "\n",
        "\n",
        "\n",
        "def word_extraction(words, labels):\n",
        "  entities = []\n",
        "  current_entity = None\n",
        "\n",
        "  for word, label in zip(words, labels):\n",
        "      if label.startswith(\"B-\"):\n",
        "          # start of a new entity\n",
        "          if current_entity is not None:\n",
        "              entities.append(current_entity)\n",
        "          current_entity = (word, label[2:])\n",
        "      elif label.startswith(\"I-\"):\n",
        "          # continuation of current entity\n",
        "          if current_entity is None:\n",
        "              current_entity = (word, label[2:])\n",
        "          else:\n",
        "              current_entity = (current_entity[0] + \" \" + word, current_entity[1])\n",
        "      else:\n",
        "          # end of current entity\n",
        "          if current_entity is not None:\n",
        "              entities.append(current_entity)\n",
        "              current_entity = None\n",
        "\n",
        "  # add last entity if exists\n",
        "  if current_entity is not None:\n",
        "      entities.append(current_entity)\n",
        "\n",
        "  # print(entities)\n",
        "  return entities"
      ],
      "metadata": {
        "id": "Z7vl8f4ruKGW",
        "outputId": "29bc1f6d-7004-4b65-eeb1-4612444fa5fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Prediction\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "def get_text_predictions(model, loader):\n",
        "    device = torch.device('cuda')\n",
        "    model = model.to(device)\n",
        "    ner_results = {}\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for data_input, _ in tqdm(loader):\n",
        "          for s in data_input:\n",
        "            tweet = s[0]\n",
        "            # print('dddd', s[0])\n",
        "            # input_ids = data_input['input_ids'].squeeze(1).to(device)\n",
        "            # tweet = data_input\n",
        "            inputs = tokenizer.encode(tweet, add_special_tokens=False, return_tensors=\"pt\", truncation=True)\n",
        "            # print('iiiiiii', inputs)\n",
        "            inputs = inputs.squeeze(1).to(device)\n",
        "            # output = model(inputs).logits\n",
        "            logits = model(inputs).logits\n",
        "            # print('llll',logits)\n",
        "            predicted_token_class_ids = logits.argmax(-1)\n",
        "            predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
        "            # print('pppp',predicted_tokens_classes)\n",
        "            entities = word_extraction(tweet.split(), predicted_tokens_classes)\n",
        "            for t in entities:\n",
        "              if t[0] not in ner_results:\n",
        "                ner_results[t[0]] = 1\n",
        "              else:\n",
        "                ner_results[t[0]] += 1\n",
        "    ner_results = {k:v for k,v in ner_results.items() if v>=5}\n",
        "    sorted_ner = dict(sorted(ner_results.items(), reverse=True, key= lambda items: items[1]))\n",
        "    print('rrrr', sorted_ner)\n",
        "    return sorted_ner"
      ],
      "metadata": {
        "id": "OH_w5lQotm3C"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_list = [test_3d, test_1w, test_after_1w]\n",
        "with open('./ner_results.csv', 'w') as f:\n",
        "    f.write('time \\n')\n",
        "    for i in range(len(time_list)):\n",
        "        if i == 0:\n",
        "            t = '0 to 3 days'\n",
        "        elif i == 1:\n",
        "            t = '4 to 7 days'\n",
        "        else:\n",
        "            t = '8 to 16 days'\n",
        "        test_dataloader = DataLoader(TweetDataset(time_list[i][:1000], tokenizer),\n",
        "                                     batch_size=8,\n",
        "                                     shuffle=False,\n",
        "                                     num_workers=0)\n",
        "    \n",
        "        r = get_text_predictions(model, test_dataloader)\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F2s-zxGuLzP",
        "outputId": "f3e79f15-7bc9-48a3-923d-5808eff7cc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry 102: Incomprehensible levels devastation despair suffering Aleppo Itâs 4Â°c tonight people still rubble still digging bare hands SaveNorthSyria earthquake TurkeySyriaEarthquake Ø§ÙÙØ°ÙØ§Ø§ÙØ´Ù Ø§ÙØ§ÙØ³ÙØ±Ù\n",
            "Entry 435: The Saudi government opened account residents transfer donations Syria Turkey Within couple hours 200000 people donated total 75 million riyals â 20 million dollars\n",
            "Entry 860: ð Earthquake deprem M34 occurred 28 km E KahramanmaraÅ Turkey 8 min ago local time 003554 More info ð± ð ð¥\n",
            "Entry 270: nonot appstoreFailure turkey problem\n",
            "Entry 106: Beware Turkey Syeria Earthquake Fake Donation Sites Please dont donate unknown organization There least 144 fake agencies various networks collecting donations None legitimate organizations Contact Turkey Syerias local embassy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:12<00:00, 12.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rrrr {'Turkey': 180, 'Syria': 162, 'TurkeySyriaEarthquake': 37, 'earthquake': 32, 'Twitter': 23, 'Turkey Syria': 21, 'Turkish': 15, 'US': 14, 'turkey': 13, 'info': 13, 'twitter': 13, 'Earthuack2023': 13, 'Syrians': 11, 'TurkeyQuake': 10, 'Syrian': 10, 'HelpTurkey': 9, 'TurkeyEarthquake': 9, 'earthquake italyearthquake': 9, 'min ago More': 8, 'Earthquake': 7, 'Turkiye Syria': 6, 'This': 6, 'people': 6, 'earthquakeinturkey': 6, 'TURKIYE': 5, 'TÃ¼rkiye': 5, 'Ukraine': 5, 'aid': 5}\n",
            "Entry 102: Heroes WhiteHelmets Syria Earthquake SyriaEarthquake\n",
            "Entry 435: 2 Istanbul 166 hours quake sondakika Turkiye 29605 died incl 2 Indonesian 89 Palestinian Suat Bayram Australian 375 Syrian 7 Iraqi Syria 5279 died Death Toll 34884 Surpassed 34000 died Nearly 35000 depremden\n",
            "Entry 860: WATCH Senior paramedic head disaster response Israels emergency medical service provides update minutes team rescued 12yearold boy 23yearold woman Turkey earthquakes aftermath TurkeySyriaEarthquake Via MDA\n",
            "Entry 270: But idea govât might misuse words push misleading narrative new dismaying Of course disgusting behavior incomprehensible scientist Welcome Turkey ruled evil mentality 20 years TurkeyQuake\n",
            "Entry 106: As world knows Iâm big fan dictators takes pleasure whitewashing genocides Today I travelled way Syria meet one âHis excellencyâ heinous record annihilating paramedics exterminating healthcare facilities\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rNIRH2hJzSHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}